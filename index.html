<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>


    <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/latest.min.js"></script> -->
   

    <title>Low-Rank Pruning of Llama2</title>
    <style>
        /* cspell:disable-file */
        /* webkit printing magic: print all background colors */
        html {
            -webkit-print-color-adjust: exact;
        }

        * {
            box-sizing: border-box;
            -webkit-print-color-adjust: exact;
        }

        html,
        body {
            margin: 0;
            padding: 0;
        }

        @media only screen {
            body {
                margin: 2em auto;
                max-width: 900px;
                color: rgb(55, 53, 47);
            }
        }

        body {
            line-height: 1.5;
            /* white-space: pre-wrap; */
        }

        a,
        a.visited {
            color: inherit;
            text-decoration: underline;
        }

        .pdf-relative-link-path {
            font-size: 80%;
            color: #444;
        }

        h1,
        h2,
        h3 {
            letter-spacing: -0.01em;
            line-height: 1.2;
            font-weight: 600;
            margin-bottom: 0;
        }

        .page-title {
            font-size: 2.5rem;
            font-weight: 700;
            margin-top: 0;
            margin-bottom: 0.75em;
        }

        h1 {
            font-size: 1.875rem;
            margin-top: 1.875rem;
        }

        h2 {
            font-size: 1.5rem;
            margin-top: 1.5rem;
        }

        h3 {
            font-size: 1.25rem;
            margin-top: 1.25rem;
        }

        .source {
            border: 1px solid #ddd;
            border-radius: 3px;
            padding: 1.5em;
            word-break: break-all;
        }

        .callout {
            border-radius: 3px;
            padding: 1rem;
        }

        figure {
            margin: 1.25em 0;
            page-break-inside: avoid;
        }

        figcaption {
            opacity: 0.5;
            font-size: 85%;
            margin-top: 0.5em;
        }

        mark {
            background-color: transparent;
        }

        .indented {
            padding-left: 1.5em;
        }

        hr {
            background: transparent;
            display: block;
            width: 100%;
            height: 1px;
            visibility: visible;
            border: none;
            border-bottom: 1px solid rgba(55, 53, 47, 0.09);
        }

        img {
            max-width: 100%;
        }

        @media only print {
            img {
                max-height: 100vh;
                object-fit: contain;
            }
        }

        @page {
            margin: 1in;
        }

        .collection-content {
            font-size: 0.875rem;
        }

        .column-list {
            display: flex;
            justify-content: space-between;
        }

        .column {
            padding: 0 1em;
        }

        .column:first-child {
            padding-left: 0;
        }

        .column:last-child {
            padding-right: 0;
        }

        .table_of_contents-item {
            display: block;
            font-size: 0.875rem;
            line-height: 1.3;
            padding: 0.125rem;
        }

        .table_of_contents-indent-1 {
            margin-left: 1.5rem;
        }

        .table_of_contents-indent-2 {
            margin-left: 3rem;
        }

        .table_of_contents-indent-3 {
            margin-left: 4.5rem;
        }

        .table_of_contents-link {
            text-decoration: none;
            opacity: 0.7;
            border-bottom: 1px solid rgba(55, 53, 47, 0.18);
        }

        table,
        th,
        td {
            border: 1px solid rgba(55, 53, 47, 0.09);
            border-collapse: collapse;
        }

        table {
            border-left: none;
            border-right: none;
        }

        th,
        td {
            font-weight: normal;
            padding: 0.25em 0.5em;
            line-height: 1.5;
            min-height: 1.5em;
            text-align: left;
        }

        th {
            color: rgba(55, 53, 47, 0.6);
        }

        ol,
        ul {
            margin: 0;
            margin-block-start: 0.6em;
            margin-block-end: 0.6em;
        }

        li>ol:first-child,
        li>ul:first-child {
            margin-block-start: 0.6em;
        }

        ul>li {
            list-style: disc;
        }

        ul.to-do-list {
            padding-inline-start: 0;
        }

        ul.to-do-list>li {
            list-style: none;
        }

        .to-do-children-checked {
            text-decoration: line-through;
            opacity: 0.375;
        }

        ul.toggle>li {
            list-style: none;
        }

        ul {
            padding-inline-start: 1.7em;
        }

        ul>li {
            padding-left: 0.1em;
        }

        ol {
            padding-inline-start: 1.6em;
        }

        ol>li {
            padding-left: 0.2em;
        }

        .mono ol {
            padding-inline-start: 2em;
        }

        .mono ol>li {
            text-indent: -0.4em;
        }

        .toggle {
            padding-inline-start: 0em;
            list-style-type: none;
        }

        /* Indent toggle children */
        .toggle>li>details {
            padding-left: 1.7em;
        }

        .toggle>li>details>summary {
            margin-left: -1.1em;
        }

        .selected-value {
            display: inline-block;
            padding: 0 0.5em;
            background: rgba(206, 205, 202, 0.5);
            border-radius: 3px;
            margin-right: 0.5em;
            margin-top: 0.3em;
            margin-bottom: 0.3em;
            white-space: nowrap;
        }

        .collection-title {
            display: inline-block;
            margin-right: 1em;
        }

        .page-description {
            margin-bottom: 2em;
        }

        .simple-table {
            margin-top: 1em;
            font-size: 0.875rem;
            empty-cells: show;
        }

        .simple-table td {
            height: 29px;
            min-width: 120px;
        }

        .simple-table th {
            height: 29px;
            min-width: 120px;
        }

        .simple-table-header-color {
            background: rgb(247, 246, 243);
            color: black;
        }

        .simple-table-header {
            font-weight: 500;
        }

        time {
            opacity: 0.5;
        }

        .icon {
            display: inline-block;
            max-width: 1.2em;
            max-height: 1.2em;
            text-decoration: none;
            vertical-align: text-bottom;
            margin-right: 0.5em;
        }

        img.icon {
            border-radius: 3px;
        }

        .user-icon {
            width: 1.5em;
            height: 1.5em;
            border-radius: 100%;
            margin-right: 0.5rem;
        }

        .user-icon-inner {
            font-size: 0.8em;
        }

        .text-icon {
            border: 1px solid #000;
            text-align: center;
        }

        .page-cover-image {
            display: block;
            object-fit: cover;
            width: 100%;
            max-height: 30vh;
        }

        .page-header-icon {
            font-size: 3rem;
            margin-bottom: 1rem;
        }

        .page-header-icon-with-cover {
            margin-top: -0.72em;
            margin-left: 0.07em;
        }

        .page-header-icon img {
            border-radius: 3px;
        }

        .link-to-page {
            margin: 1em 0;
            padding: 0;
            border: none;
            font-weight: 500;
        }

        p>.user {
            opacity: 0.5;
        }

        td>.user,
        td>time {
            white-space: nowrap;
        }

        input[type="checkbox"] {
            transform: scale(1.5);
            margin-right: 0.6em;
            vertical-align: middle;
        }

        p {
            margin-top: 0.5em;
            margin-bottom: 0.5em;
        }

        .image {
            border: none;
            margin: 1.5em 0;
            padding: 0;
            border-radius: 0;
            text-align: center;
        }

        .code,
        code {
            background: rgba(135, 131, 120, 0.15);
            border-radius: 3px;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 85%;
            tab-size: 2;
        }

        code {
            color: #eb5757;
        }

        .code {
            padding: 1.5em 1em;
        }

        .code-wrap {
            white-space: pre-wrap;
            word-break: break-all;
        }

        .code>code {
            background: none;
            padding: 0;
            font-size: 100%;
            color: inherit;
        }

        blockquote {
            font-size: 1.25em;
            margin: 1em 0;
            padding-left: 1em;
            border-left: 3px solid rgb(55, 53, 47);
        }

        .bookmark {
            text-decoration: none;
            max-height: 8em;
            padding: 0;
            display: flex;
            width: 100%;
            align-items: stretch;
        }

        .bookmark-title {
            font-size: 0.85em;
            overflow: hidden;
            text-overflow: ellipsis;
            height: 1.75em;
            white-space: nowrap;
        }

        .bookmark-text {
            display: flex;
            flex-direction: column;
        }

        .bookmark-info {
            flex: 4 1 180px;
            padding: 12px 14px 14px;
            display: flex;
            flex-direction: column;
            justify-content: space-between;
        }

        .bookmark-image {
            width: 33%;
            flex: 1 1 180px;
            display: block;
            position: relative;
            object-fit: cover;
            border-radius: 1px;
        }

        .bookmark-description {
            color: rgba(55, 53, 47, 0.6);
            font-size: 0.75em;
            overflow: hidden;
            max-height: 4.5em;
            word-break: break-word;
        }

        .bookmark-href {
            font-size: 0.75em;
            margin-top: 0.25em;
        }

        .sans {
            font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
        }

        .code {
            font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace;
        }

        .serif {
            font-family: Lyon-Text, Georgia, ui-serif, serif;
        }

        .mono {
            font-family: iawriter-mono, Nitti, Menlo, Courier, monospace;
        }

        .pdf .sans {
            font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP';
        }

        .pdf:lang(zh-CN) .sans {
            font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC';
        }

        .pdf:lang(zh-TW) .sans {
            font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC';
        }

        .pdf:lang(ko-KR) .sans {
            font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR';
        }

        .pdf .code {
            font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP';
        }

        .pdf:lang(zh-CN) .code {
            font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC';
        }

        .pdf:lang(zh-TW) .code {
            font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC';
        }

        .pdf:lang(ko-KR) .code {
            font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR';
        }

        .pdf .serif {
            font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP';
        }

        .pdf:lang(zh-CN) .serif {
            font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC';
        }

        .pdf:lang(zh-TW) .serif {
            font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC';
        }

        .pdf:lang(ko-KR) .serif {
            font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR';
        }

        .pdf .mono {
            font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP';
        }

        .pdf:lang(zh-CN) .mono {
            font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC';
        }

        .pdf:lang(zh-TW) .mono {
            font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC';
        }

        .pdf:lang(ko-KR) .mono {
            font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR';
        }

        .highlight-default {
            color: rgba(55, 53, 47, 1);
        }

        .highlight-gray {
            color: rgba(120, 119, 116, 1);
            fill: rgba(120, 119, 116, 1);
        }

        .highlight-brown {
            color: rgba(159, 107, 83, 1);
            fill: rgba(159, 107, 83, 1);
        }

        .highlight-orange {
            color: rgba(217, 115, 13, 1);
            fill: rgba(217, 115, 13, 1);
        }

        .highlight-yellow {
            color: rgba(203, 145, 47, 1);
            fill: rgba(203, 145, 47, 1);
        }

        .highlight-teal {
            color: rgba(68, 131, 97, 1);
            fill: rgba(68, 131, 97, 1);
        }

        .highlight-blue {
            color: rgba(51, 126, 169, 1);
            fill: rgba(51, 126, 169, 1);
        }

        .highlight-purple {
            color: rgba(144, 101, 176, 1);
            fill: rgba(144, 101, 176, 1);
        }

        .highlight-pink {
            color: rgba(193, 76, 138, 1);
            fill: rgba(193, 76, 138, 1);
        }

        .highlight-red {
            color: rgba(212, 76, 71, 1);
            fill: rgba(212, 76, 71, 1);
        }

        .highlight-gray_background {
            background: rgba(241, 241, 239, 1);
        }

        .highlight-brown_background {
            background: rgba(244, 238, 238, 1);
        }

        .highlight-orange_background {
            background: rgba(251, 236, 221, 1);
        }

        .highlight-yellow_background {
            background: rgba(251, 243, 219, 1);
        }

        .highlight-teal_background {
            background: rgba(237, 243, 236, 1);
        }

        .highlight-blue_background {
            background: rgba(231, 243, 248, 1);
        }

        .highlight-purple_background {
            background: rgba(244, 240, 247, 0.8);
        }

        .highlight-pink_background {
            background: rgba(249, 238, 243, 0.8);
        }

        .highlight-red_background {
            background: rgba(253, 235, 236, 1);
        }

        .block-color-default {
            color: inherit;
            fill: inherit;
        }

        .block-color-gray {
            color: rgba(120, 119, 116, 1);
            fill: rgba(120, 119, 116, 1);
        }

        .block-color-brown {
            color: rgba(159, 107, 83, 1);
            fill: rgba(159, 107, 83, 1);
        }

        .block-color-orange {
            color: rgba(217, 115, 13, 1);
            fill: rgba(217, 115, 13, 1);
        }

        .block-color-yellow {
            color: rgba(203, 145, 47, 1);
            fill: rgba(203, 145, 47, 1);
        }

        .block-color-teal {
            color: rgba(68, 131, 97, 1);
            fill: rgba(68, 131, 97, 1);
        }

        .block-color-blue {
            color: rgba(51, 126, 169, 1);
            fill: rgba(51, 126, 169, 1);
        }

        .block-color-purple {
            color: rgba(144, 101, 176, 1);
            fill: rgba(144, 101, 176, 1);
        }

        .block-color-pink {
            color: rgba(193, 76, 138, 1);
            fill: rgba(193, 76, 138, 1);
        }

        .block-color-red {
            color: rgba(212, 76, 71, 1);
            fill: rgba(212, 76, 71, 1);
        }

        .block-color-gray_background {
            background: rgba(241, 241, 239, 1);
        }

        .block-color-brown_background {
            background: rgba(244, 238, 238, 1);
        }

        .block-color-orange_background {
            background: rgba(251, 236, 221, 1);
        }

        .block-color-yellow_background {
            background: rgba(251, 243, 219, 1);
        }

        .block-color-teal_background {
            background: rgba(237, 243, 236, 1);
        }

        .block-color-blue_background {
            background: rgba(231, 243, 248, 1);
        }

        .block-color-purple_background {
            background: rgba(244, 240, 247, 0.8);
        }

        .block-color-pink_background {
            background: rgba(249, 238, 243, 0.8);
        }

        .block-color-red_background {
            background: rgba(253, 235, 236, 1);
        }

        .select-value-color-interactiveBlue {
            background-color: rgba(35, 131, 226, .07);
        }

        .select-value-color-pink {
            background-color: rgba(245, 224, 233, 1);
        }

        .select-value-color-purple {
            background-color: rgba(232, 222, 238, 1);
        }

        .select-value-color-green {
            background-color: rgba(219, 237, 219, 1);
        }

        .select-value-color-gray {
            background-color: rgba(227, 226, 224, 1);
        }

        .select-value-color-translucentGray {
            background-color: rgba(255, 255, 255, 0.0375);
        }

        .select-value-color-orange {
            background-color: rgba(250, 222, 201, 1);
        }

        .select-value-color-brown {
            background-color: rgba(238, 224, 218, 1);
        }

        .select-value-color-red {
            background-color: rgba(255, 226, 221, 1);
        }

        .select-value-color-yellow {
            background-color: rgba(253, 236, 200, 1);
        }

        .select-value-color-blue {
            background-color: rgba(211, 229, 239, 1);
        }

        .select-value-color-pageGlass {
            background-color: undefined;
        }

        .select-value-color-washGlass {
            background-color: undefined;
        }

        .checkbox {
            display: inline-flex;
            vertical-align: text-bottom;
            width: 16;
            height: 16;
            background-size: 16px;
            margin-left: 2px;
            margin-right: 5px;
        }

        .checkbox-on {
            background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
        }

        .checkbox-off {
            background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
        }
    </style>
</head>

<body>
    <article id="b0f92218-e672-4c1f-a44d-8b5369550c15" class="page sans">
        <header>
            <h1 class="page-title">Low-Rank Pruning of Llama2</h1>
            
        </header>
        <div class="page-body">
            <p id="ca06c94f-113d-4350-9f1d-3784d3ff1a0c" class=""><a href="https://scholar.google.com/citations?user=LxweMX4AAAAJ&hl=en"><mark
                        class="highlight-gray">Hicham Badri</mark></a><mark class="highlight-gray">, </mark><a
                    href="https://scholar.google.com/citations?user=HxZDDzUAAAAJ&hl=en"><mark class="highlight-gray">Appu Shaji</mark></a><mark
                    class="highlight-gray"></mark></p>
                    <p><mark class="highlight-gray">Mobius Labs GmbH</mark></p>
            <hr id="b060532c-9954-4b51-8fda-d07da8aa383b" />
            <p id="7fc3842a-638f-40a0-ad38-613ff019fa6e" class="">In the ever-evolving landscape of artificial intelligence (AI), one undeniable trend has emerged in recent years: the relentless growth in the size and complexity of machine learning models. More specifically, large language models (LLMs) that mainly rely on transformers as building blocks, are reaching a substantial number of parameters and require a significant amount of compute that is expected to increase with larger and larger models being released. 
            </p>
            <p id="3fcf90e4-f19a-4617-9d9d-b8de9f8dc8d3" class="">In this article, we explore low-rankness as a pruning technique of the LLama2-7B base model. We show that, by splitting almost all the linear layer weights into low-rank pairs <em>without fine-tuning</em> and leveraging LoRA for custom training, we can achieve the following without <em>implementing custom kernels</em>:
                <ul>
                    <li>~50% reduction in the number of parameters.</li>
                    <li>Up to  ~50% faster training vs. bitsandbytes’s 8-bit quantization.</li>
                    <li>Up to ~1.25x inference speed-up.</li>
                </ul>

            </p>
            <!-- <p id="c8835517-e8ec-4781-8d42-047d63df4d94" class=""><strong>Paper</strong>: <a
                    href="https://arxiv.org/abs/2310.06694">https://arxiv.org/abs/2310.06694</a>
                <strong>Code</strong>: <a
                    href="https://github.com/princeton-nlp/LLM-Shearing">https://github.com/princeton-nlp/LLM-Shearing</a>
                <strong>Models</strong>: <a
                    href="https://huggingface.co/princeton-nlp/Sheared-LLaMA-1.3B">Sheared-LLaMA-1.3B</a>, <a
                    href="https://huggingface.co/princeton-nlp/Sheared-LLaMA-2.7B">Sheared-LLaMA-2.7B</a>
            </p> -->
            <hr id="2723ddd6-073a-4bfb-ab0b-b8c73212c290" />
            <div id="7d921bae-a457-483b-a01d-e3317d6ba238" class="column-list">
                <div id="59ac2e7a-85a1-46a3-a409-3c338efe58a0" style="width:32%" class="column">
                    <p id="73f8efb1-f832-438d-b4ad-29d4a3d58bff" class="">
                    </p>
                    <!-- <p class="page-description"><img src="./baby_aana.png" /></p> -->
                    <figure id="c580bcb8-ffc8-4fbc-a61a-1cbcfe98c566" class="image" style="text-align:left"><a
                            href="figs/baby_aana.png"><img
                                style="width:240px"
                                src="figs/baby_aana.png" /></a>
                    </figure>
                    <p id="2595eb7c-e99d-4aa7-b0f2-83c7a5d1203b" class="">
                        <strong><strong><strong><strong><strong><strong><strong>Table of
                                                    Contents</strong></strong></strong></strong></strong></strong></strong>
                    </p>
                    <nav id="b3fb94d2-802c-48dd-bb18-9d1c007a4e18" class="block-color-gray table_of_contents">
                        <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
                                href="#intro">Introduction</a></div>
                        <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
                                href="#lowrankpruning">Low-Rank Pruning</a>
                        </div>
                        <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
                                href="#pruningllama2">Low-Rank Pruning of Llama2 Models</a></div>                        
                        <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
                                href="benchmark">Speed Benchmarks</a></div>
                        <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
                                href="#dataset">Dataset Performance</a></div>
                        <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
                                href="conclusion">Conclusion</a></div>
                        <!-- <div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link"
                                href="#291a3097-c118-4f5d-aad4-76df5b0640bf">Downstream Tasks</a></div>
                        <div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link"
                                href="#64334576-0a31-4c45-8248-6ead541cd98f">Instruction Tuning</a></div>
                        <div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link"
                                href="#57f58e54-f393-44e7-963d-ba340b88aa71">Continual Pre-Training</a></div>
                        <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link"
                                href="#1cdc85eb-cad1-4dc1-8647-1bfb9b9267fc">Consider using it!</a></div> -->
                    </nav>

                </div>
                <div id="b94ff864-b6c7-4fc4-b84b-c193e4b009b0" style="width:75%" class="column">
                    <h2 id="intro" class="">Introduction</h2>
                    <p>Model pruning refers to the process of removing redundant information from machine learning models to make them “leaner”. As a result, the pruned model is smaller in size and should run faster which is suitable for deployment on resource-constrained devices or in real-time applications. Pruning can be combined with other techniques such as quantization to further optimize runtime. The most popular pruning approaches are based on discarding neurons, layer channels or entire layers. This kind of pruning is referred to as “sparsification”. 
                    </p>

                    <p>In practice however, sparse pruning has many limitations. In order to achieve actual speed-up in practice, custom sparsity-aware matrix multiplication (matmul) operations are required. For the moment, this is only partially supported in Ampere GPUs (https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/) or on CPUs via NeuralMagic https://neuralmagic.com/ . In Pytorch, sparse matrix multiplication operations are not optimized. For example, there is no implementation available of the batched matmul operation with sparse matrices. Rewriting it with the existing operation requires some reshaping and the result is 2-3x slower performance.  
                    </p>

                    <p>Structured sparsity on the other hand consists in discarding weights in a structured way. For instance, we can remove columns, remove channels, block matrices, etc. This way, in theory, the model can be pruned without requiring specialized software/hardware for optimized runtime. Some structured sparsity methods still require optimized software to achieve faster runtime. For example, block-sparsity requires implementing dedicated GPU kernels for block-sparse matmul such as https://openai.com/research/block-sparse-gpu-kernels .
                    </p>

                    <p>In practice however, structured sparsity cannot be pushed too far without a larger drop in accuracy compared to unstructured sparsity. As a result, the performance gain is usually very limited. 
                    </p>


                
           

                    <h2 id="lowrankpruning" class="">Low Rank Pruning</h2>
                    The idea of low-rank pruning revolves around factorizing the weight matrix W of a linear layer as a matrix multiplication of a pair of two matrices A and B, such that A and B have much less columns and rows respectively:

                    <figure id="low_rank"><img style="width:480px" src="figs/Matrix2.png" /></figure>
                    
                    <p>Ideally, we would like the chain matmul operation with A and B to be faster and take less memory, while the overall model prediction stays as close as possible to the original prediction with unaltered weights. We refer to the number of columns of A/number of rows of B as the maximum rank (denoted by max_rank) in the rest of the article. </p>

                    <p>There are various ways to achieve such a factorization (SVD, QR, etc.). We use the SVD decomposition as follows to get the matrix pairs: </p>

                    <figure><img style="width:480px;" src="figs/get_lowrank_tuple.png" /></figure>

                    
                    <p>The idea of using low-rankness is not new in the context of Transformer models.  The adoption of low-rank estimation has garnered considerable attention, primarily within the domain of model compression. The works in <a href="https://arxiv.org/pdf/2004.04124.pdf"> https://arxiv.org/pdf/2004.04124.pdf</a> and <a href="https://neurips2022-enlsp.github.io/papers/paper_33.pdf">https://neurips2022-enlsp.github.io/papers/paper_33.pdf</a> study low-rank compression of BERT and GPT models, respectively. An additional approach, documented in <a href="https://openreview.net/pdf?id=uPv9Y3gmAI5">https://openreview.net/pdf?id=uPv9Y3gmAI5</a>, employs weighted low-rank estimation to compress BERT-based models. Furthermore, the research outlined in <a href="https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf>https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf"https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf>https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf</a> explores an innovative perspective by focusing on low-rank compression of the model features, as opposed to the model weights.</p>

                    <p>Among these approaches, one that has gained significant popularity is <a href="https://arxiv.org/abs/2106.09685">LoRA (Low-Rank Adaptation)</a>. LoRA's core concept revolves around training supplementary low-rank parameters to adapt large models. This technique enables the training of expansive models while drastically reducing the memory requirements. </p>

                    <p>Pruning typically requires fine-tuning on a large dataset, which is very expensive even for smaller LLM models such as LLama2-7B. We find that, by applying low-rank estimation, freezing the weights and leveraging LoRA instead for custom training, we can achieve significant efficiency as we explain in the next section.
                    </p>


                    <h2 id="pruningllama2" class="">Low-Rank Pruning of Llama2 Models</h2>
                    <p>When we analyze the weights of the Llama2-7B model, we find that many are in fact already low-rank, especially those of the attention layers (Q,K,O). The graph x shows the distribution of the average normalized singular values per layer type. We normalize the singular values by the highest value (which is the same as normalizing the matrix weight by its L2 norm) so we can average the singular values across the layers and get a single plot per layer type. We can clearly see that most of the energy is concentrated in a subset of the singular values. More specifically, about 80% of the energy is concentrated in the first half of the singular values of the Q,K,V,O layers of the attention modules. The first layers of the attention module tend to have an even lower-rank. For instance, 88% of the energy of the first Q layer is concentrated in the first 1024 (25%) of its singular values.</p>

                    <figure><img style="width:480px;" src="figs/svd_distribution.png"/></figure>


                    <p>In practice, we found that the rank of the original attention and mlp layers can be reduced from 4096 to 1024 and 2048 respectively, while still delivering good performance after LoRA training. This is a 4x rank reduction in the attention layers and 2x for the MLP layers, which is quite aggressive given that these weights are frozen after pruning. 
                    </p>

                    <p>We summarize the steps for training and inference using the proposed approach:
                    </p>


                    <h4>Training Mode</h4>
                    <ul>
                        <li>For each linear layer, we run SVD on the weights of the linear layers <b>W</b> to get the <b>A</b>,<b>B</b> matrix pairs such that <b>AB</b>estimates <b>W</b> using the predefined max_rank value to truncate the singular values as explained in the previous section. The only layer that we keep full-rank is the <b>v_proj</b>. This is because the rank of the weights of this layer tends to be higher.</li>
                        <li>We freeze all the weights and use LoRA with the r parameter to create the new trainable parameters.
                        </li>
                    </ul>

                    <h4>Inference mode</h4>
                    <p>After training, we need to re-estimate new pairs of matrices that combine the original low-rank weights and the newly trained LoRA weights:</p>
                    <ul>
                        <li>For each linear layer that was pruned, we have the <b>A</b>,<b>B</b> as well as the LoRA pairs that we refer to as <b>AL</b>,<b>BL</b> </li>
                        <li><a href="https://www.ic.unicamp.br/~meidanis/PUB/Doutorado/2012-Biller/Marsaglia1964.pdf">Since the rank of the sum of two matrices is lower or equal than the sum of their ranks</a> 
                            $$ {rank({\bf AB}+{\bf A_L} {\bf B_L} ) \le rank({\bf AB}) + rank({\bf A_LB_L})} $$ 
                            we can safely combine the 4 weights by applying truncated SVD on the sum of their matrix multiplications using the sum of their ranks to build the new low-rank pair:
                            $$ {{\bf AB} + {\bf A_LB_L}  \Rightarrow {\bf \bar{A} \bar{B} }} $$
                            $$ { rank({\bf AB}) = max rank + r } $$

                        </li>
                        <li>Now we can use the new pairs and remove the older A,B and LoRA weights. 
                        </li>
                    </ul>
                    

                    <p>The illustration below shows the difference between the standard LoRA approach and the proposed low-rank LoRA merging method. Note that the result is a pair of matrices.</p>


                    <figure><img style="width:480px" src="figs/merging.png" /></figure>

                    <p>The code below summarizes the merging logic:</p>

                    <figure><img style="width:640px" src="figs/pseudo-code.png" /></figure>

                    <h2 id="benchmark">Speed Benchmark</h2>

                    <p>We report the inference speed-up in comparison to the original LLama2-7B model. We employ the HuggingFace implementations with fp16 precision. When we merge the LoRA weights into the original model, the resulting matrices maintain the same dimensions as those in the original model. However, in the pruned version, the rank of the matrices increases by the LoRA rank r. For instance, in the attention layers, the initial weight matrix W has dimensions of 4096x4096. By using a rank of 2048 and a LoRA rank of 32, the resulting pairs A and B will be 4096x2080 and 2080x4096, respectively. Reducing the rank leads to a faster speed boost but has a detrimental effect on prediction accuracy.</p>


                    <figure style="display:flex; align-items: center; justify-content: center;">
                        <img style="margin-right: 10px; max-width: 100%; height: auto;" src="figs/titan.png" />
                        <img style="margin-right: 10px; max-width: 100%; height: auto;" src="figs/a100.png" />
                    </figure>

                    <h2 id="dataset">Dataset Performance</h2>
                    <p>We present performance results on 5 datasets, evaluating both the unaltered and pruned LLama2-7B models using the perplexity metric. In the case of the original model, we use the default LoRA settings (r=8). Conversely, in the pruned model, we raise the LoRA rank to 32. Remarkably, the pruned model exhibits strong performance despite the removal of approximately half of the original weights, all without any fine-tuning!</p>

                    <p>It is worth noting that the performance of the pruned model on OpenOrca-1M is better than that of the original model on 100k samples. This indicates that the pruned model has the capacity to learn but needs more samples to compensate for the noise introduced by pruning.</p>

                    <table>
                        <tr>
                            <td><b>Dataset</b></td>
                            <td><b>LLama2-7B</b></td>
                            <td><b>LLama2-7B pruned</b></td>
                        </tr>
                        <tr>
                            <td>vicgalle/alpaca-gpt4</td>
                            <td>3.49</td>
                            <td>4.11</td>                                   
                        </tr>
                        <tr>
                            <td>databricks/databricks-dolly-15k</td>
                            <td>4.13</td>
                            <td>5.86</td>
                        </tr>
                        <tr>
                            <td>knkarthick/dialogsum</td>
                            <td>3.78</td>
                            <td>4.82</td>
                        </tr>
                        <tr>
                            <td>ArtifactAI/arxiv-math-instruct-50k</td>
                            <td>3.08</td>
                            <td>3.73</td>
                        </tr>
                        <tr>
                            <td>Open-Orca/OpenOrca - 100k </td>
                            <td>3.51</td>
                            <td>4.27</td>
                        </tr>
                        <tr>
                            <td>Open-Orca/OpenOrca - 1M </td>
                            <td>-</td>
                            <td>3.43</td>
                        </tr>
                        <tr>
                            <td><i>Average</i></td>
                            <td><i>3.60</i></td>
                            <td><i>4.56</i></td>
                        </tr>

                    </table>


                    <h2 id="conclusion">Conclusion</h2>

                    <p>In this article, we've demonstrated the utility of low-rank pruning as an effective method for accelerating large language models like LLama2-7B. Unlike sparse pruning, which often requires custom hardware or software configurations to realize significant speed gains, low-rank pruning doesn't require specialized kernel operations and can seamlessly integrate with existing matrix multiplication (<i>matmul</i>) implementations.
                    </p>

                    <p>Nevertheless, there is ample scope for further refinements, and we aspire for this article to serve as an inspiration to the research community. We encourage researchers to embrace low-rank pruning and explore its synergistic potential when combined with other pruning and quantization techniques. 
                    </p>


                    <p>We provide code examples at <a href="https://github.com/mobiusml/low-rank-llama2/tree/main/code">https://github.com/mobiusml/low-rank-llama2/tree/main/code</a>
                    </p>




                </div>
    
    






            </div>
            <p id="d9be7859-86c8-4e9e-8957-b0127ad9431d" class="">
            <div class="indented">
                <p id="7b0d7f13-0909-4e80-97fe-e0102053cc62" class="">
                </p>
            </div>
            </p>
        </div>
    </article>
</body>

</html>